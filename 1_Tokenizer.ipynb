{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM55T7fwM6yMikU5+UqIm56",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomAmster/Data-Science/blob/master/1_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81hv7z1isHfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import seaborn as sns \n",
        "import numpy as np\n",
        "import re\n",
        "import hashlib\n",
        "from spacy.attrs import ORTH\n",
        "from spacy.attrs import LOWER\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0CWIsKLlZc_",
        "colab_type": "text"
      },
      "source": [
        "# 0.Introduction & Overview\n",
        "This Notebooks perform the tokenization processing of the note summaries\n",
        "1. Define Important Abbreviation\n",
        "2. Define Custom Entities\n",
        "3. Define Regex Custom Rules\n",
        "4. Use Spacy to Tokenize Each Note Event"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eaatX9DLWGX",
        "colab_type": "text"
      },
      "source": [
        "# 1.Define Tokenizer Utility Functions\n",
        "Tokenizing Functions To Tokenize The Note Summaries Events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8P6TwsNLpyf",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Define Medical Abberviation Patterns\n",
        "We use list of medical abbreviation taken from Wikipedia\n",
        "\n",
        "*   https://en.wikipedia.org/wiki/List_of_medical_abbreviations:_Latin_abbreviations\n",
        "*  https://en.wikipedia.org/wiki/List_of_abbreviations_used_in_medical_prescriptions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5GMyZu5LkS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "abbrev_pattern1 = ['a.c.','A.c.','A.C.','a.c.h.s.','A.c.h.s.','A.C.H.S.','ac&hs', 'Ac&hs','AC&HS','a.d.','A.d.','A.D.','ad.', 'Ad.', 'AD.', 'add.', 'Add.', 'ADD.', 'admov.', 'Admov.', 'ADMOV.', 'aeq.', 'Aeq.', 'AEQ.', 'agit.', 'Agit.', 'AGIT.', 'amp.', 'Amp.', 'AMP.', 'aq.', 'Aq.', 'AQ.', 'a.l.', 'A.l.', 'A.L.', 'a.s.', 'A.s.', 'A.S.', 'a.u.', 'A.u.', 'A.U.', 'b.d.s.', 'B.d.s.', 'B.D.S.', 'bib.', 'Bib.', 'BIB.', 'b.i.d.', 'B.i.d.', 'B.I.D.', 'b.d.', 'B.d.', 'B.D.', 'bol.', 'Bol.', 'BOL.', 'ph.br.', 'Ph.br.', 'PH.BR.', 'b.t.', 'B.t.', 'B.T.', 'bucc.', 'Bucc.', 'BUCC.', 'cap.', 'Cap.', 'CAP.', 'caps.', 'Caps.', 'CAPS.', 'c.m.', 'C.m.', 'C.M.', 'c.m.s.', 'C.m.s.', 'C.M.S.', 'c.', 'C.', 'C.', 'cib.', 'Cib.', 'CIB.', 'c.c.', 'C.c.', 'C.C.', 'cf.', 'Cf.', 'CF.', 'c.n.', 'C.n.', 'C.N.', 'cochl.', 'Cochl.', 'COCHL.', 'colet.', 'Colet.', 'COLET.', 'comp.', 'Comp.', 'COMP.', 'contin.', 'Contin.', 'CONTIN.', 'cpt.', 'Cpt.', 'CPT.', 'cr.', 'Cr.', 'CR.', 'cuj.', 'Cuj.', 'CUJ.', 'c.v.', 'C.v.', 'C.V.', 'cyath.', 'Cyath.', 'CYATH.', 'd.', 'D.', 'D.', 'd/c', 'D/c', 'D/C', 'decoct.', 'Decoct.', 'DECOCT.', 'det.', 'Det.', 'DET.', 'dil.', 'Dil.', 'DIL.', 'dim.', 'Dim.', 'DIM.', 'disp.', 'Disp.', 'DISP.', 'div.', 'Div.', 'DIV.', 'd.t.d. .', 'D.t.d. .', 'D.T.D. .', 'elix.', 'Elix.', 'ELIX.', 'e.m.p.', 'E.m.p.', 'E.M.P.', 'emuls.', 'Emuls.', 'EMULS.', 'exhib.', 'Exhib.', 'EXHIB.', 'f.', 'F.', 'F.', 'f.h.', 'F.h.', 'F.H.', 'fl.', 'Fl.', 'FL.', 'fld.', 'Fld.', 'FLD.', 'f.m.', 'F.m.', 'F.M.', 'f.s.a.', 'F.s.a.', 'F.S.A.', 'ft.', 'Ft.', 'FT.', 'garg.', 'Garg.', 'GARG.', 'gr.', 'Gr.', 'GR.', 'gtt.', 'Gtt.', 'GTT.', 'gutt.', 'Gutt.', 'GUTT.', 'h.', 'H.', 'H.', 'h/o', 'H/o', 'H/O', 'hor.', 'Hor.', 'HOR.', 'habt.', 'Habt.', 'HABT.', 'h.s.', 'H.s.', 'H.S.', 'inj.', 'Inj.', 'INJ.', 'i.m.', 'I.m.', 'I.M.', 'inf.', 'Inf.', 'INF.', 'i.v.', 'I.v.', 'I.V.', 'i.v.p.', 'I.v.p.', 'I.V.P.', 'lb.', 'Lb.', 'LB.', 'l.c.d.', 'L.c.d.', 'L.C.D.', 'liq.', 'Liq.', 'LIQ.', 'lot.', 'Lot.', 'LOT.', 'm.', 'M.', 'M.', 'm.', 'M.', 'M.', 'max.', 'Max.', 'MAX.', 'm.d.u.', 'M.d.u.', 'M.D.U.', 'mg/dl', 'Mg/dl', 'MG/DL', 'min.', 'Min.', 'MIN.', 'mist.', 'Mist.', 'MIST.', 'mit.', 'Mit.', 'MIT.', 'mitt.', 'Mitt.', 'MITT.', 'nebul.', 'Nebul.', 'NEBUL.', 'neb.', 'Neb.', 'NEB.', 'noct.', 'Noct.', 'NOCT.', 'n.p.o.', 'N.p.o.', 'N.P.O.', '1/2ns.', '1/2ns.', '1/2NS.', 'o.d.', 'O.d.', 'O.D.', 'o.m.', 'O.m.', 'O.M.', 'o.n.', 'O.n.', 'O.N.', 'o.s.', 'O.s.', 'O.S.', 'o.u.', 'O.u.', 'O.U.', 'p.', 'P.', 'P.', 'p.c.', 'P.c.', 'P.C.', 'p.c.h.s.', 'P.c.h.s.', 'P.C.H.S.', 'pc&hs', 'Pc&hs', 'PC&HS', 'ph.br.', 'Ph.br.', 'PH.BR.', 'ph.eur.', 'Ph.eur.', 'PH.EUR.', 'ph.int.', 'Ph.int.', 'PH.INT.', 'pig.', 'Pig.', 'PIG.', 'pigm.', 'Pigm.', 'PIGM.', 'p.o.', 'P.o.', 'P.O.', 'ppt.', 'Ppt.', 'PPT.', 'p.r.', 'P.r.', 'P.R.', 'p.r.n.', 'P.r.n.', 'P.R.N.', 'pt.', 'Pt.', 'PT.', 'pulv.', 'Pulv.', 'PULV.', 'p.v.', 'P.v.', 'P.V.', 'q.1.d.', 'Q.1.d.', 'Q.1.D.', 'q.1.h.', 'Q.1.h.', 'Q.1.H.', 'q.2.h.', 'Q.2.h.', 'Q.2.H.', 'q.4.h.', 'Q.4.h.', 'Q.4.H.', 'q.6.h.', 'Q.6.h.', 'Q.6.H.', 'q.8.h.', 'Q.8.h.', 'Q.8.H.', 'q.a.d.', 'Q.a.d.', 'Q.A.D.', 'q.a.m.', 'Q.a.m.', 'Q.A.M.', 'q.d.', 'Q.d.', 'Q.D.', 'q.d.a.m.', 'Q.d.a.m.', 'Q.D.A.M.', 'q.d.p.m.', 'Q.d.p.m.', 'Q.D.P.M.', 'q.d.s.', 'Q.d.s.', 'Q.D.S.', 'q.p.m.', 'Q.p.m.', 'Q.P.M.', 'q.h.', 'Q.h.', 'Q.H.', 'q.h.s.', 'Q.h.s.', 'Q.H.S.', 'q.i.d.', 'Q.i.d.', 'Q.I.D.', 'q.l.', 'Q.l.', 'Q.L.', 'q.n.', 'Q.n.', 'Q.N.', 'q.o.d.', 'Q.o.d.', 'Q.O.D.', 'q.p.m.', 'Q.p.m.', 'Q.P.M.', 'q.q.', 'Q.q.', 'Q.Q.', 'q.q.h.', 'Q.q.h.', 'Q.Q.H.', 'q.s.', 'Q.s.', 'Q.S.', 'q.v.', 'Q.v.', 'Q.V.', 'rel.', 'Rel.', 'REL.', 'rel.', 'Rel.', 'REL.', 'rep.', 'Rep.', 'REP.', 'rept.', 'Rept.', 'REPT.', 'r/l', 'R/l', 'R/L', 'rep.', 'Rep.', 'REP.', 's.', 'S.', 'S.', 's.a.', 'S.a.', 'S.A.', 'sem.', 'Sem.', 'SEM.', 's.i.d.', 'S.i.d.', 'S.I.D.', 'sig.', 'Sig.', 'SIG.', 's.', 'S.', 'S.', 'sig.', 'Sig.', 'SIG.', 'sing.', 'Sing.', 'SING.', 's.l.', 'S.l.', 'S.L.', 'sol.', 'Sol.', 'SOL.', 's.o.s.', 'S.o.s.', 'S.O.S.', 's.s.', 'S.s.', 'S.S.', 'st.', 'St.', 'ST.', 'stat.', 'Stat.', 'STAT.', 'sum.', 'Sum.', 'SUM.', 'supp.', 'Supp.', 'SUPP.', 'susp.', 'Susp.', 'SUSP.', 'sust.rel.sust.rel.sust.rel.', 'Sust.rel.sust.rel.sust.rel.', 'SUST.REL.SUST.REL.SUST.REL.', 'sust.rel.', 'Sust.rel.', 'SUST.REL.', 'sust.', 'Sust.', 'SUST.', 'sust', 'Sust', 'SUST', 'syr.', 'Syr.', 'SYR.', 'tab.', 'Tab.', 'TAB.', 'tal.', 'Tal.', 'TAL.', 't.', 'T.', 'T.', 't.d.s.', 'T.d.s.', 'T.D.S.', 't.i.d.', 'T.i.d.', 'T.I.D.', 't.d.', 'T.d.', 'T.D.', 't.d.s.', 'T.d.s.', 'T.D.S.', 'tinct.', 'Tinct.', 'TINCT.', 't.i.d.', 'T.i.d.', 'T.I.D.', 't.i.w.', 'T.i.w.', 'T.I.W.', 'top.', 'Top.', 'TOP.', 'tinc.', 'Tinc.', 'TINC.', 'tinct.', 'Tinct.', 'TINCT.', 'trit.', 'Trit.', 'TRIT.', 'troch.', 'Troch.', 'TROCH.', 'u.d.', 'U.d.', 'U.D.', 'ut.', 'Ut.', 'UT.', 'dict.', 'Dict.', 'DICT.', 'ung.', 'Ung.', 'UNG.', 'vag.', 'Vag.', 'VAG.', 'vinos.', 'Vinos.', 'VINOS.', 'w/a', 'W/a', 'W/A', 'w/f', 'W/f', 'W/F', 'w/o', 'W/o', 'W/O', 'y.o.', 'Y.o.', 'Y.O.']\n",
        "abbrev_pattern2 = ['alt. d.', 'alt. dieb.', 'alt. h.', 'alt. hor.', 'aq. bull.', 'aq. com.', 'aq. dest.', 'aq. ferv.', 'cochl. ampl.', 'cochl. infant.', 'cochl. mag.', 'cochl. mod.', 'cochl. parv.', 'dieb. alt.', 'f. pil.', 'hor. alt.', 'hor. decub.', 'hor. intermed.', 'hor. tert.', 'lat. dol.', 'mod. praescript.', 'omn. bih.', 'omn. hor.', 'part. aeq.' ]\n",
        "abbrev_pattern3 = ['ad lib.', 'ad us.', 'bis ind.', 'ex aq.', 'non rep.' ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QN-rrvMRCaa",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Defining Patttern Matching Generators Based on Abberviations Structures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "771rDIyLRQbf",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.1 Abberviation Entitities To be Used As Wholee\n",
        "Single Letters Abberviations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWuhtBhHRYy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_matcher_pattern1():\n",
        "\tfor abbrev in abbrev_pattern1:\n",
        "\t\tyield (abbrev, [{ORTH: abbrev}])\n",
        "\t\t\t\t\t\t\t\t\t\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMW6WJGlRbHE",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.2 Formatted 2 Words Abbreviations Splitted By A Period\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt4cHlHPRvZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_matcher_pattern2():\n",
        "\tfor abbrev in abbrev_pattern2:\n",
        "\t\tw1, w2 = [w.strip() for w in abbrev.split('.')][:2]\n",
        "\t\tw1 = w1.lower()\n",
        "\t\tw2 = w2.lower()\n",
        "\t\tyield ('p2.1_{}_{}'.format(w1, w2), [{LOWER: w1}, {ORTH: '.'}, {LOWER: w2}, {ORTH: '.'}])\n",
        "\t\tyield ('P2.2_{}_{}'.format(w1, w2), [{LOWER: w1 + '.'}, {LOWER: w2}, {ORTH: '.'}])\n",
        "\t\tyield ('p2.3_{}_{}'.format(w1, w2), [{LOWER: w1}, {ORTH: '.'}, {LOWER: w2 + '.'}])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aecDeMLERZgY",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.3 Formatted 2 Words Abbreviations Ending With Period"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYtfFCfhSCWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_matcher_pattern3():\n",
        "\tfor abbrev in abbrev_pattern3:\n",
        "\t\tw1, w2 = [w.strip() for w in abbrev.split(' ')]\n",
        "\t\tw1 = w1.lower()\n",
        "\t\tw2 = w2.lower().strip('.')\n",
        "\t\tyield ('p3_{}_{}'.format(w1, w2), [{LOWER: w1}, {LOWER: w2}, {ORTH: '.'}])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0zIcXouLikS",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Definining Entities For NER Tagging and Replacements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itHXrB2dTlM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ANONTOKEN = 'ANONTOKEN'\n",
        "FIRST_NAME_TOKEN = 'FIRSTNAMETOKEN'\n",
        "LAST_NAME_TOKEN = 'LASTNAMETOKEN'\n",
        "DOCTOR_FIRST_NAME_TOKEN = 'DOCTORFIRSTNAMETOKEN'\n",
        "DOCTOR_LAST_NAME_TOKEN = 'DOCTORLASTNAMETOKEN'\n",
        "NAME_TOKEN = 'NAMETOKEN'\n",
        "NAME_PREFIX_TOKEN = 'NAMEPREFIXTOKEN'\n",
        "ADDRESS_TOKEN = 'ADDRESSTOKEN'\n",
        "LOCATION_TOKEN = 'LOCATIONTOKEN'\n",
        "HOSPITAL_TOKEN = 'HOSPITALTOKEN'\n",
        "PO_BOX_TOKEN = 'POBOXTOKEN'\n",
        "STATE_TOKEN = 'STATENAMETOKEN'\n",
        "COUNTRY_TOKEN = 'COUNTRYNAMETOKEN'\n",
        "COMPANY_TOKEN = 'COMPANYNAMETOKEN'\n",
        "TELEPHONE_NUMBER_TOKEN = 'TELEPHONENUMBERTOKEN'\n",
        "PAGER_NUMBER_TOKEN = 'PAGERNUMBERTOKEN'\n",
        "SSN_TOKEN = 'SSNTOKEN'\n",
        "MEDICAL_RECORD_NUMBER_TOKEN = 'MEDICALRECORDNUMBERTOKEN'\n",
        "UNIT_NUMBER_TOKEN = 'UNITNUMBERTOKEN'\n",
        "AGE_OVER_90_TOKEN = 'AGEOVER90TOKEN'\n",
        "EMAIL_ADDRESS_TOKEN = 'EMAILADDRESSTOKEN'\n",
        "URL_TOKEN = 'URLADDRESSTOKEN'\n",
        "HOLYDAY_TOKEN = 'HOLYDAYNAMETOKEN'\n",
        "JOB_NUMBER_TOKEN = 'JOBNUMBERTOKEN'\n",
        "MD_NUMBER_TOKEN = 'MDNUMBERTOKEN'\n",
        "DATE_RANGE_TOKEN = 'DATERANGETOKEN'\n",
        "NUMERIC_IDENTIFIER_TOKEN = 'NUMERICIDENTIFIERTOKEN'\n",
        "DATE_LITERAL_TOKEN = 'DATETOKEN'\n",
        "UNIVERSITY_TOKEN = 'UNIVERSITYTOKEN'\n",
        "DICTATOR_INFO_TOKEN = 'DICTATORINFOTOKEN'\n",
        "CC_CONTACT_INFO_TOKEN = 'CCCONTACTINFOTOKEN'\n",
        "CLIP_NUMBER_TOKEN = 'CLIPNUMBERTOKEN'\n",
        "SERIAL_NUMBER_TOKEN = 'SERIALNUMBERTOKEN'\n",
        "ATTENDING_INFO_TOKEN = 'ATTENDINGINFOTOKEN'\n",
        "PROVIDER_NUMBER_TOKEN = 'PROVIDERNUMBERTOKEN'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1pWjmS6UviB",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Define RegEX Rules for RE library to bsed use to identify words for tagging replacement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPPUiWdITlDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str_month_year = r'(?P<month_year>(?P<month__month_year>[0-9]{1,2})-/(?P<year__month_year>[0-9]{4}))'\n",
        "str_month_day = r'(?P<month_day>(?P<month__month_day>[0-9]{1,2})-(?P<day__month_day>[0-9]{1,2}))'\n",
        "str_year_month_day = r'(?P<year_month_day>(?P<year__year_month_day>[0-9]{4})-(?P<month__year_month_day>[0-9]{1,2})-(?P<day__year_month_day>[0-9]{1,2}))'\n",
        "str_year = r'(?P<year>[0-9]{4})'\n",
        "str_date = r'|'.join([str_month_year, str_month_day, str_year_month_day, str_year])\n",
        "str_anon_date = r'(?P<anon_date>(' + str_date + '))'\n",
        "#str_anon_date = r'(?P<anon_date>([0-9/\\-]+)|([0-9]+))'\n",
        "str_anon_first_name = r'(?P<anon_first_name>(Known firstname)|(Female First Name)|(Male First Name)|(First Name))'\n",
        "str_anon_last_name = r'(?P<anon_last_name>(Known lastname)|(Last Name))'\n",
        "str_anon_doctor_first_name = r'(?P<anon_doctor_first_name>(Doctor First Name))'\n",
        "str_anon_doctor_last_name = r'(?P<anon_doctor_last_name>(Doctor Last Name))'\n",
        "str_anon_name = r'(?P<anon_name>(Name)|(Name Initial)|(Initials)|(Initial))'\n",
        "str_anon_name_prefix = r'(?P<anon_name_prefix>(Name Prefix))'\n",
        "str_anon_address = r'(?P<anon_address>(Street Address)|(Apartment Address))'\n",
        "str_anon_university = r'(?P<anon_university>(Location \\(Universities\\))|(University/College))' #keep before location\n",
        "str_anon_location = r'(?P<anon_location>(Location))'\n",
        "str_anon_hospital = r'(?P<anon_hospital>(Hospital)|(Wardname)|(Hospital Unit Name)|(Hospital Ward Name))'\n",
        "str_anon_po_box = r'(?P<anon_po_box>(PO BOX)|(PO Box))'\n",
        "str_anon_state = r'(?P<anon_state>(State)|(State/Zipcode))'\n",
        "str_anon_country = r'(?P<anon_country>(Country))'\n",
        "str_anon_company = r'(?P<anon_company>(Company))'\n",
        "str_anon_telephone_number = r'(?P<anon_telephone_number>(Telephone/Fax))'\n",
        "str_anon_pager_number = r'(?P<anon_pager_number>(Pager number))'\n",
        "str_anon_social_security_number = r'(?P<anon_social_security_number>(Social Security Number))'\n",
        "str_anon_medical_record_number = r'(?P<anon_medical_record_number>(Medical Record Number))'\n",
        "str_anon_unit_number = r'(?P<anon_unit_number>(Unit Number))'\n",
        "str_anon_age_over_90 = r'(?P<anon_age_over_90>(Age over 90))'\n",
        "str_anon_email_address = r'(?P<anon_email_address>(E-mail address))'\n",
        "str_anon_url = r'(?P<anon_url>(URL))'\n",
        "str_anon_holiday = r'(?P<anon_holiday>(Holiday))'\n",
        "str_anon_job_number = r'(?P<anon_job_number>(Job Number))'\n",
        "str_anon_md_number = r'(?P<anon_md_number>(MD Number))'\n",
        "str_anon_date_range = r'(?P<anon_date_range>(Date range)|(Date Range))'\n",
        "str_anon_numeric_identifier = r'(?P<anon_numeric_identifier>(Numeric Identifier))'\n",
        "str_anon_date_literal = r'(?P<anon_date_literal>(Month)|(Month/Day)|(Month/Year)|(Year)|(Month/Day/Year)|(Year/Month/Day)|(Year/Month)|(Day Month)|(Month Day)|(Month Year)|(Month/Year 1)|(January)|(February)|(March)|(April)|(May)|(June)|(July)|(August)|(September)|(October)|(November)|(December))'\n",
        "str_anon_dictator_info = r'(?P<anon_dictator_info>(Dictator Info))'\n",
        "str_anon_cc_contact_info = r'(?P<anon_cc_contact_info>(CC Contact Info))'\n",
        "str_anon_clip_number = r'(?P<anon_clip_number>(Clip Number))'\n",
        "str_anon_serial_number = r'(?P<anon_serial_number>(Serial Number))'\n",
        "str_anon_attending_info = r'(?P<anon_attending_info>(Attending Info))'\n",
        "str_anon_provider_number = r'(?P<anon_provider_number>(Provider Number))'\n",
        "str_anon_default = r'(.*?)'\n",
        "str_anon_tokens = r'|'.join([str_anon_first_name, str_anon_last_name, str_anon_doctor_first_name, str_anon_doctor_last_name,\n",
        "\t\t\t\t\t\t\t\t\t\tstr_anon_name, str_anon_name_prefix, str_anon_address, str_anon_university, str_anon_location, str_anon_hospital,\n",
        "\t\t\t\t\t\t\t\t\t\tstr_anon_po_box, str_anon_state, str_anon_country, str_anon_company,\n",
        "\t\t\t\t\t\t\t\t\t\tstr_anon_telephone_number, str_anon_pager_number, str_anon_social_security_number, str_anon_medical_record_number,\n",
        "\t\t\t\t\t\t\t\t\t\tstr_anon_unit_number, str_anon_age_over_90, str_anon_email_address, str_anon_url, str_anon_holiday, str_anon_job_number,\n",
        "\t\t\t\t\t\t\t\t\t\tstr_anon_md_number, str_anon_date_range, str_anon_numeric_identifier, str_anon_date_literal, str_anon_dictator_info,\n",
        "\t\t\t\t\t\t\t\t\t\tstr_anon_cc_contact_info, str_anon_clip_number, str_anon_serial_number, str_anon_attending_info, str_anon_provider_number,\n",
        "\t\t\t\t\t\t\t\t\t\tstr_anon_default])\n",
        "\t\t\t\t\t\t\t\t\t\t\n",
        "str_anon = r'(?P<anon>\\[\\*\\*(({date})|(({tokens})(\\(?[0-9]+\\)?)?((\\s?(\\(.*?\\)\\s)?)|(\\s))(?P<anon_id>[0-9]+)?))\\*\\*\\])'.format(date=str_anon_date, tokens=str_anon_tokens)\n",
        "regex_anon = re.compile(str_anon)\n",
        "\n",
        "regex_anon_boundaries = re.compile(r'\\[\\*\\*.*?\\*\\*\\]')\n",
        "\n",
        "# start of a numbered section, such as a list, but with no whitespace\n",
        "# separating the numbers from the adjacent text\n",
        "str_list_start_no_space = r'(?P<list_start_no_space>\\b(?P<listnum>\\d+(\\.|\\)))(?P<word>[a-zA-Z]+))'\n",
        "regex_list_start_no_space = re.compile(str_list_start_no_space)\n",
        "\n",
        "# find numbered sentences: look for digits followed by '.' or ')',\n",
        "# whitespace, then a capital letter starting a word\n",
        "str_list_start = r'(?P<list_start>\\b(?P<listnum>\\d+(\\.|\\)))\\s+)'\n",
        "str_list_item = r'(?P<list_item>' + str_list_start + r'([A-Z][a-z]+|\\d)\\b)'\n",
        "regex_list_start = re.compile(str_list_start)\n",
        "regex_list_item  = re.compile(str_list_item)\n",
        "\n",
        "# find captialized headers\n",
        "str_caps_word = r'\\b([123]-?D|[-_A-Z]+|[-_A-Z]+/[-_A-Z]+)\\b'\n",
        "str_caps_header = r'(?P<caps_header>(' + str_caps_word + r'\\s+)*' + str_caps_word + r'\\s*#?:)'\n",
        "regex_caps_header = re.compile(str_caps_header)\n",
        "\n",
        "# find concatenated sentences with no space after the period\n",
        "\n",
        "# need at least two chars before '.', to avoid matching C.Diff, M.Smith, etc.\n",
        "# neg lookahead prevents capturing inside abbreviations such as Sust.Rel.\n",
        "str_two_sentences = r'(?P<two_sentences>\\b[a-zA-Z]{2,}\\.[A-Z][a-z]+(?!\\.))'\n",
        "regex_two_sentences = re.compile(str_two_sentences)\n",
        "\n",
        "regex_multi_space = re.compile(r' +')\n",
        "regex_multi_newline = re.compile(r'\\n+')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YekNqtbgV9hY",
        "colab_type": "text"
      },
      "source": [
        "## 1.5 Remove Newlines & Spaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKR-5pvwV17l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_newlines(document):\n",
        "\n",
        "\t\t# replace newline with space\n",
        "\t\tno_newlines = regex_multi_newline.sub(' ', document)\n",
        "\n",
        "\t\t# replace multiple consecutive spaces with single space\n",
        "\t\tcleaned_document = regex_multi_space.sub(' ', no_newlines)\n",
        "\t\treturn cleaned_document"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ok0sbYsWE-P",
        "colab_type": "text"
      },
      "source": [
        "## 1.6 Generate Tokens Rules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi2B7pTCWL5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def generate_token(base_name, mo, mode):\n",
        "\t\n",
        "\tif mode == 0:\n",
        "\t\treturn ANONTOKEN\n",
        "\telif mode == 1:\n",
        "\t\treturn base_name\n",
        "\telif mode == 2:\n",
        "\t\tif mo.group('anon_id'):\n",
        "\t\t\treturn '{0}_{1}'.format(base_name, hashlib.md5(mo.group('anon').encode()).hexdigest())\n",
        "\t\telse:\n",
        "\t\t\treturn base_name\n",
        "\t\t\t\n",
        "def merge_anon_tokens(doc):\n",
        "\tmatches = regex_anon_boundaries.finditer(doc.text)\n",
        "\tif matches == None:\n",
        "\t\treturn doc\n",
        "\tfor mo in matches:\n",
        "\t\tdoc.merge(mo.start(), mo.end())\n",
        "\treturn doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4xTk40jWP53",
        "colab_type": "text"
      },
      "source": [
        "## 1.7 Lookup Dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYVffGc4WS_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "month_lookup = {1:'january',\n",
        "                2:'february',\n",
        "                3:'march',\n",
        "                4:'april',\n",
        "                5:'may',\n",
        "                6:'june',\n",
        "                7:'july',\n",
        "                8:'august',\n",
        "                9:'september',\n",
        "                10:'october',\n",
        "                11:'november',\n",
        "                12:'december',\n",
        "               }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W88txbIwWbY6",
        "colab_type": "text"
      },
      "source": [
        "## 1.8 Perform Tag Substitution Based on Regex Matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhNPVy5HWhb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_substitutions(anon_token):\n",
        "\n",
        "\tmode = 1\n",
        "\t\n",
        "\tdef repl(mo):\n",
        "\t\t\n",
        "\t\tnonlocal mode\n",
        "\t\n",
        "\t\ttext = mo.string[mo.start():mo.end()]\n",
        "\t\t\n",
        "\t\tif mo.group('anon') and mo.group('anon_date'):\n",
        "\t\t\tif mo.group('month_year'):\n",
        "\t\t\t\treturn month_lookup[int(mo.group('month__month_year'))]\n",
        "\t\t\telif mo.group('month_day'):\n",
        "\t\t\t\treturn month_lookup[int(mo.group('month__month_day'))]\n",
        "\t\t\telif mo.group('year_month_day'):\n",
        "\t\t\t\treturn month_lookup[int(mo.group('month__year_month_day'))]\n",
        "\t\t\telif mo.group('year'):\n",
        "\t\t\t\treturn mo.group('year')\n",
        "\t\telif mo.group('anon') and mo.group('anon_first_name'):\n",
        "\t\t\treturn generate_token(FIRST_NAME_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_last_name'):\n",
        "\t\t\treturn generate_token(LAST_NAME_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_doctor_first_name'):\n",
        "\t\t\treturn generate_token(DOCTOR_FIRST_NAME_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_doctor_last_name'):\n",
        "\t\t\treturn generate_token(DOCTOR_LAST_NAME_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_name'):\n",
        "\t\t\treturn generate_token(NAME_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_name_prefix'):\n",
        "\t\t\treturn generate_token(NAME_PREFIX_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_address'):\n",
        "\t\t\treturn generate_token(ADDRESS_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_university'):\n",
        "\t\t\treturn generate_token(UNIVERSITY_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_location'):\n",
        "\t\t\treturn generate_token(LOCATION_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_hospital'):\n",
        "\t\t\treturn generate_token(HOSPITAL_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_po_box'):\n",
        "\t\t\treturn generate_token(PO_BOX_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_state'):\n",
        "\t\t\treturn generate_token(STATE_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_country'):\n",
        "\t\t\treturn generate_token(COUNTRY_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_company'):\n",
        "\t\t\treturn generate_token(COMPANY_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_telephone_number'):\n",
        "\t\t\treturn generate_token(TELEPHONE_NUMBER_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_pager_number'):\n",
        "\t\t\treturn generate_token(PAGER_NUMBER_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_social_security_number'):\n",
        "\t\t\treturn generate_token(SSN_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_medical_record_number'):\n",
        "\t\t\treturn generate_token(MEDICAL_RECORD_NUMBER_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_unit_number'):\n",
        "\t\t\treturn generate_token(UNIT_NUMBER_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_age_over_90'):\n",
        "\t\t\treturn generate_token(AGE_OVER_90_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_email_address'):\n",
        "\t\t\treturn generate_token(EMAIL_ADDRESS_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_url'):\n",
        "\t\t\treturn generate_token(URL_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_holiday'):\n",
        "\t\t\treturn generate_token(HOLYDAY_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_job_number'):\n",
        "\t\t\treturn generate_token(JOB_NUMBER_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_md_number'):\n",
        "\t\t\treturn generate_token(MD_NUMBER_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_date_range'):\n",
        "\t\t\treturn generate_token(DATE_RANGE_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_numeric_identifier'):\n",
        "\t\t\treturn generate_token(NUMERIC_IDENTIFIER_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_date_literal'):\n",
        "\t\t\treturn generate_token(DATE_LITERAL_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_dictator_info'):\n",
        "\t\t\treturn generate_token(DICTATOR_INFO_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_cc_contact_info'):\n",
        "\t\t\treturn generate_token(CC_CONTACT_INFO_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_clip_number'):\n",
        "\t\t\treturn generate_token(CLIP_NUMBER_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_serial_number'):\n",
        "\t\t\treturn generate_token(SERIAL_NUMBER_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_attending_info'):\n",
        "\t\t\treturn generate_token(ATTENDING_INFO_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon') and mo.group('anon_provider_number'):\n",
        "\t\t\treturn generate_token(PROVIDER_NUMBER_TOKEN, mo, mode)\n",
        "\t\telif mo.group('anon'):\n",
        "\t\t\treturn generate_token(ANONTOKEN, mo, mode)\n",
        "\n",
        "\tanon_token = regex_anon.sub(repl, anon_token)\n",
        "\n",
        "\treturn anon_token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knNa3H-vWpKA",
        "colab_type": "text"
      },
      "source": [
        "## 1.9 Cleaning Auxillary Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84tbncHxWbJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def erase_spans(report, span_list):\n",
        "\t\"\"\"\n",
        "\tErase all report chars bounded by each [start, end) span.\n",
        "\t\"\"\"\n",
        "\n",
        "\tif len(span_list) > 0:\n",
        "\t\tprev_end = 0\n",
        "\t\tnew_report = ''\n",
        "\t\tfor span in span_list:\n",
        "\t\t\tstart = span[0]\n",
        "\t\t\tend   = span[1]\n",
        "\t\t\tnew_report += report[prev_end:start]\n",
        "\t\t\tprev_end = end\n",
        "\t\tnew_report += report[prev_end:]\n",
        "\t\treport = new_report\n",
        "\n",
        "\treturn report\n",
        "\n",
        "sub1_regex = re.compile(r'[-_*]{3,}')\n",
        "sub3_regex = re.compile(u'(\\u2018|\\u2019)')\n",
        "\n",
        "def cleanup_report(report):\n",
        "\n",
        "\t# remove (Over) ... (Cont) inserts\n",
        "\tspans = []\n",
        "\titerator = re.finditer(r'\\(Over\\)', report)\n",
        "\tfor match_over in iterator:\n",
        "\t\tstart = match_over.start()\n",
        "\t\tchunk = report[match_over.end():]\n",
        "\t\tmatch_cont = re.search(r'\\(Cont\\)', chunk)\n",
        "\t\tif match_cont:\n",
        "\t\t\tend = match_over.end() + match_cont.end()\n",
        "\t\t\tspans.append( (start, end))\n",
        "\t\t\t\n",
        "\treport = erase_spans(report, spans)\n",
        "\n",
        "\t# insert a space between list numbers and subsequent text, makes\n",
        "\t# lists and start-of-sentence negations easier to identify\n",
        "\tprev_end = 0\n",
        "\tnew_report = ''\n",
        "\titerator = regex_list_start_no_space.finditer(report)\n",
        "\tfor match in iterator:\n",
        "\t\t# end of list num (digits followed by '.' or ')'\n",
        "\t\tend = match.end('listnum')\n",
        "\t\t# start of following (concatenated) word\n",
        "\t\tstart = match.start('word')\n",
        "\t\tnew_report += report[prev_end:end]\n",
        "\t\tnew_report += ' '\n",
        "\t\tprev_end = start\n",
        "\tnew_report += report[prev_end:]\n",
        "\treport = new_report\n",
        "\n",
        "\t# remove numbering in lists\n",
        "\tspans = []\n",
        "\titerator = regex_list_item.finditer(report)\n",
        "\tfor match in iterator:\n",
        "\t\tstart = match.start('listnum')\n",
        "\t\tend   = match.end('listnum')\n",
        "\t\tspans.append( (start, end))\n",
        "\n",
        "\treport = erase_spans(report, spans)\n",
        "\t\t\n",
        "\t# Remove long runs of dashes, underscores, or stars\n",
        "\treport = sub1_regex.sub(' ', report)\n",
        "\n",
        "\t# convert unicode left and right quotation marks to ascii\n",
        "\treport = sub3_regex.sub(\"'\", report)\n",
        "\t\n",
        "\treturn report\n",
        "\t\n",
        "def fixup_sentences(sentence_list):\n",
        "\t\"\"\"\n",
        "\tMove punctuation from one sentence to another, if necessary.\n",
        "\t\"\"\"\n",
        "\t\n",
        "\tnum = len(sentence_list)\n",
        "\t\n",
        "\ti = 1\n",
        "\twhile i < num:\n",
        "\t\ts = sentence_list[i]\n",
        "\t\tif s.startswith(':') or s.startswith(','):\n",
        "\t\t\t# move to end of previous sentence\n",
        "\t\t\tsprev = sentence_list[i-1]\n",
        "\t\t\tsentence_list[i-1] = sprev + ':'\n",
        "\t\t\tsentence_list[i]   = s[1:].lstrip()\n",
        "\t\ti += 1\n",
        "\treturn sentence_list\n",
        "  def split_section_headers(doc):\n",
        "\t\"\"\"\n",
        "\tPut an all caps section header in a separate sentence from the subsequent\n",
        "\ttext.\n",
        "\t\"\"\"\n",
        "\n",
        "\tmatches = regex_caps_header.finditer(doc[:-2].text)\n",
        "\t\n",
        "\tfor mo in matches:\n",
        "\t\tprint(mo.string[mo.start():mo.end()])\n",
        "\t\tdoc[mo.end()+1].sent_start = True\n",
        "\n",
        "\treturn doc\n",
        "\t\t\n",
        "\n",
        "def split_concatenated_sentences(sentence_list):\n",
        "\n",
        "\t#sentences = []\n",
        "\tfor s in sentence_list:\n",
        "\t\tmatch = regex_two_sentences.search(s)\n",
        "\t\tif match:\n",
        "\t\t\ts1 = s[:match.end()]\n",
        "\t\t\ts2 = s[match.end():]\n",
        "\t\t\t#sentences.append(s1)\n",
        "\t\t\tyield s1\n",
        "\t\t\t#sentences.append(s2)\n",
        "\t\t\tyield s2\n",
        "\t\telse:\n",
        "\t\t\t#sentences.append(s)\n",
        "\t\t\tyield s\n",
        "\n",
        "\t#return sentences\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "def delete_junk(sentence_list):\n",
        "\t\"\"\"\n",
        "\t\"\"\"\n",
        "\n",
        "\tsentences = []\n",
        "\tnum = len(sentence_list)\n",
        "\n",
        "\tfor s in sentence_list:\n",
        "\t\ti = 0\n",
        "\t\twhile i < num:\n",
        "\t\t\ts = sentence_list[i]\n",
        "\n",
        "\t\t\t# delete any remaining list numbering\n",
        "\t\t\tmatch = regex_list_start.match(s)\n",
        "\t\t\tif match:\n",
        "\t\t\t\ts = s[match.end():]\n",
        "\n",
        "\t\t\t# remove any sentences that consist of just '1.', '2.', etc.\n",
        "\t\t\tmatch = re.match(r'\\A\\s*\\d+(\\.|\\))\\s*\\Z', s)\n",
        "\t\t\tif match:\n",
        "\t\t\t\ti += 1\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t# remove any sentences that consist of '#1', '#2', etc.\n",
        "\t\t\tmatch = re.match(r'\\A\\s*#\\d+\\s*\\Z', s)\n",
        "\t\t\tif match:\n",
        "\t\t\t\ti += 1\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t# remove any sentences consisting entirely of symbols\n",
        "\t\t\tmatch = re.match(r'\\A\\s*[^a-zA-Z0-9]+\\s*\\Z', s)\n",
        "\t\t\tif match:\n",
        "\t\t\t\ti += 1\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t# merge isolated age + year\n",
        "\t\t\tif i < num-1:\n",
        "\t\t\t\tif s.isdigit() and sentence_list[i+1].startswith('y'):\n",
        "\t\t\t\t\ts = s + ' ' + sentence_list[i+1]\n",
        "\t\t\t\t\ti += 1\n",
        "\n",
        "\t\t\t# if next sentence starts with 'now measures', merge with current\n",
        "\t\t\tif i < num-1:\n",
        "\t\t\t\tif sentence_list[i+1].startswith('now measures'):\n",
        "\t\t\t\t\ts = s + ' ' + sentence_list[i+1]\n",
        "\t\t\t\t\ti += 1\n",
        "\n",
        "\t\t\tsentences.append(s)\n",
        "\t\t\ti += 1\n",
        "\n",
        "\treturn sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jozzUR91W5XF",
        "colab_type": "text"
      },
      "source": [
        "# 2.Defining Tokenizer Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z7Xmar7ssf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sudo pip install jsonlines\n",
        "import jsonlines\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpAF-k0cf9-r",
        "colab_type": "code",
        "outputId": "63149eeb-8b42-458b-bc59-a90402bdbdd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkcsOBjamlnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/Shared drives/NLP Seminar')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw9MqpatXBoP",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Download Spacy Pre Trained Model For Tagging, Parsing and NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXpAJIHegvZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!python -m spacy download en_core_web_md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg2sC1ixf7HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import en_core_web_md as english_model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_RcbcOff2w8",
        "colab_type": "code",
        "outputId": "0349ef74-c74c-44ac-94ec-687c09878810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "import string\n",
        "from timeit import default_timer as timer\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer as SpacyTokenizer\n",
        "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex, update_exc\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "import tokenizer_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTYw1IpIjZa_",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Define A Tokenizer Class Using Spacy\n",
        "1. Define Spacy to Perfrom Tokenization Only\n",
        "2. Use Matching Patterns defined above as exception for Regular Expressions\n",
        "3. Load Spacy Matcher with our custom rules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPiGbRHAhpfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tokenizer(object):\n",
        "\n",
        "\tdef __init__(self, batch_size, n_cpus, n_threads, mode):\n",
        "\n",
        "\t\tprint('loading model...', end=' ')\n",
        "\t\tself.nlp = spacy.load('en_core_web_md')\n",
        "    #self.nlp = english_model.load()\n",
        "\t\tself.nlp.remove_pipe('tagger')\n",
        "\t\tself.nlp.remove_pipe('ner')\n",
        "\t\t\n",
        "\t\tpunct = list(string.punctuation)\n",
        "\t\tpunct.remove('.')\n",
        "\t\tpunct.append('[**')\n",
        "\t\tpunct.append('**]')\n",
        "\t\tpunct = [re.escape(p) for p in punct]\n",
        "\t\t\n",
        "\t\tprefixes_custom = tuple(punct)\n",
        "\t\tinfixes_custom = tuple(punct)\n",
        "\t\tsuffixes_custom = tuple(punct)\n",
        "\n",
        "\t\texceptions_custom = {id : pattern for id, pattern in tokenizer_utils.generate_matcher_pattern1()}\t\t\n",
        "\t\texceptions = update_exc(self.nlp.Defaults.tokenizer_exceptions, exceptions_custom)\n",
        "\n",
        "\t\tprefix_re = compile_prefix_regex(self.nlp.Defaults.prefixes + prefixes_custom)\n",
        "\t\tinfix_re  = compile_infix_regex(infixes_custom + self.nlp.Defaults.infixes)\n",
        "\t\tsuffix_re = compile_suffix_regex(self.nlp.Defaults.suffixes + suffixes_custom)\n",
        "\t\t\n",
        "\t\ttokenizer = SpacyTokenizer(self.nlp.vocab, rules=exceptions,\n",
        "\t\t\t\t\t\t\tprefix_search=prefix_re.search,\n",
        "\t\t\t\t\t\t\tsuffix_search=suffix_re.search,\n",
        "\t\t\t\t\t\t\tinfix_finditer=infix_re.finditer, token_match=self.nlp.Defaults.token_match)\n",
        "\n",
        "\t\tself.nlp.tokenizer = tokenizer\n",
        "\n",
        "\t\tmatcher = Matcher(self.nlp.vocab)\n",
        "\t\t\t\t\t\t\n",
        "\t\tdef on_match_pattern(matcher, doc, id, matches):\n",
        "\t\t\n",
        "\t\t\tmatch_id, start, end = matches[id]\n",
        "\n",
        "\t\t\tif self.nlp.vocab.strings[match_id].startswith('p3'):\n",
        "\t\t\t\tspan = doc[start+1:end]\n",
        "\t\t\t\tspan.merge()\n",
        "\t\t\t\tfor i in range(id, len(matches)):\n",
        "\t\t\t\t\tmatches[i] = (matches[i][0], matches[i][1] - 1,  matches[i][2] - 1)\n",
        "\n",
        "\t\t\telif self.nlp.vocab.strings[match_id].startswith('p2.1'):\n",
        "\t\t\t\tspan1 = doc[start:start+2]\n",
        "\t\t\t\tspan2 = doc[start+2:end]\n",
        "\t\t\t\tspan1.merge()\n",
        "\t\t\t\tspan2.merge()\n",
        "\t\t\t\tfor i in range(id, len(matches)):\n",
        "\t\t\t\t\tmatches[i] = (matches[i][0], matches[i][1] - 2,  matches[i][2] - 2)\n",
        "\n",
        "\t\t\telif self.nlp.vocab.strings[match_id].startswith('p2.2'):\n",
        "\t\t\t\tspan2 = doc[start+1:end]\n",
        "\t\t\t\tspan2.merge()\n",
        "\t\t\t\tfor i in range(id, len(matches)):\n",
        "\t\t\t\t\tmatches[i] = (matches[i][0], matches[i][1] - 1,  matches[i][2] - 1)\n",
        "\n",
        "\t\t\telif self.nlp.vocab.strings[match_id].startswith('p2.3'):\n",
        "\t\t\t\tspan1 = doc[start:start+2]\n",
        "\t\t\t\tspan1.merge()\n",
        "\t\t\t\tfor i in range(id, len(matches)):\n",
        "\t\t\t\t\tmatches[i] = (matches[i][0], matches[i][1] - 1,  matches[i][2] - 1)\n",
        "\t\n",
        "\t\tfor id, pattern in tokenizer_utils.generate_matcher_pattern2():\n",
        "\t\t\tmatcher.add(id, on_match_pattern, pattern)\n",
        "\t\t\t\n",
        "\t\tfor id, pattern in tokenizer_utils.generate_matcher_pattern3():\n",
        "\t\t\tmatcher.add(id, on_match_pattern, pattern)\n",
        "\t\t\t\t\n",
        "\t\tself.nlp.add_pipe(matcher, before='parser')\n",
        "\n",
        "\t\tprint('done')\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.n_cpus = n_cpus\n",
        "\t\tself.n_threads = n_threads\n",
        "\t\tself.mode = mode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5mijN9qk-Fd",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Define Note-Even Summary Tokenization Procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgoRPj5sk_Rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\tdef tokenize_documents(self, documents):\n",
        "\n",
        "\t\tprint('start parsing')\n",
        "\t\tstart = timer()\n",
        "\t\tprint('\\tcleaning and tokenization...')\n",
        "\n",
        "\t\tdocuments = (tokenizer_utils.cleanup_report(doc) for doc in documents)\n",
        "\t\tdocuments = (tokenizer_utils.merge_anon_tokens(doc) for doc in self.nlp.pipe(documents, n_threads=self.n_threads, batch_size=self.batch_size))\n",
        "\n",
        "\t\tdocs = [[[token.lower() for token in [tokenizer_utils.do_substitutions(word.text) for word in sentence] if any(char.isalpha() for char in token) ] for sentence in doc.sents] for doc in documents]\n",
        "\t\t\n",
        "\t\tend = timer()\n",
        "\t\tprint('\\tdone ({0:.2f}s)'.format(end-start))\n",
        "\n",
        "\t\treturn docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsxNylK-lIz-",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Define Batch Processing Procedure Due to Dataset Size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFMWWhuMixy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_batch(notes_batch, i, tokenizer, notes_tokenized_file):\n",
        "\tprint('processing batch {0}, batchsize {1}'.format(i, 100))\n",
        "\n",
        "\tdocuments = tokenizer.tokenize_documents(notes_batch['TEXT'])\n",
        "\tn_tokens = [sum(len(sentence) for sentence in doc) for doc in documents]\n",
        "\trows = list(zip(notes_batch['SUBJECT_ID'], notes_batch['HADM_ID'].fillna(-1).astype(np.int32), notes_batch['CHARTDATE'].fillna('1970-01-01'), notes_batch['CATEGORY'], notes_batch['DESCRIPTION'], notes_batch['ISERROR'].fillna(0).astype(bool), n_tokens, documents))\n",
        "\t#print('\\n'.join(token for doc in documents for sentence in doc for token in sentence))\n",
        "\n",
        "\tfor row in rows:\n",
        "\t\tnotes_tokenized_file.write(row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul-wsQ2xlSc8",
        "colab_type": "text"
      },
      "source": [
        "# 2.5 Perfrom and Invoke Tokenization of Note Events"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnU-xhN4r-DU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_reader = pd.read_csv('/content/drive/Shared drives/NLP Seminar/Data/NOTEEVENTS.csv', chunksize=100, usecols=['SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CATEGORY', 'DESCRIPTION', 'ISERROR','TEXT'], dtype={'SUBJECT_ID':np.int32, 'HADM_ID': 'str', 'CATEGORY': 'str', 'DESCRIPTION':'str', 'ISERROR':'str', 'TEXT':'str'}, keep_default_na=False, na_values='')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-Abd0OIsAiw",
        "colab_type": "code",
        "outputId": "5fd66d6e-628a-4fb3-97b1-331891298749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "with jsonlines.open('/content/drive/Shared drives/NLP Seminar/Data/notes_tokenized_file.ndjson', 'w') as notes_tokenized_file:\n",
        "  n_cpus=os.cpu_count()\n",
        "  n_threads =n_cpus*4  \n",
        "  tokenizer = Tokenizer(100, n_cpus, n_threads, 2)\n",
        "  for i, notes_batch in enumerate(csv_reader):\n",
        "    process_batch(notes_batch, i, tokenizer, notes_tokenized_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading model... done\n",
            "processing batch 0, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (131.67s)\n",
            "processing batch 1, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (131.57s)\n",
            "processing batch 2, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (119.53s)\n",
            "processing batch 3, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (129.05s)\n",
            "processing batch 4, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (158.14s)\n",
            "processing batch 5, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (140.14s)\n",
            "processing batch 6, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (153.38s)\n",
            "processing batch 7, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (118.59s)\n",
            "processing batch 8, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (129.33s)\n",
            "processing batch 9, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (126.71s)\n",
            "processing batch 10, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (125.42s)\n",
            "processing batch 11, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (134.13s)\n",
            "processing batch 12, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (131.69s)\n",
            "processing batch 13, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (131.63s)\n",
            "processing batch 14, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (139.67s)\n",
            "processing batch 15, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (132.61s)\n",
            "processing batch 16, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (128.40s)\n",
            "processing batch 17, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (125.05s)\n",
            "processing batch 18, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (138.82s)\n",
            "processing batch 19, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (124.19s)\n",
            "processing batch 20, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (128.52s)\n",
            "processing batch 21, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (137.44s)\n",
            "processing batch 22, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (122.90s)\n",
            "processing batch 23, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (114.96s)\n",
            "processing batch 24, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (131.13s)\n",
            "processing batch 25, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (129.74s)\n",
            "processing batch 26, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n",
            "\tdone (121.54s)\n",
            "processing batch 27, batchsize 100\n",
            "start parsing\n",
            "\tcleaning and tokenization...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF7Mg2fTItbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"finished Running\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}